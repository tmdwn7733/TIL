# 선형대수
연속적인 숫자들을 대신할수 있는 많은 양의 데이터
> 곱셈과 덧셈은 왜할까?
>> 덧셈 : 상위개념 요약(연관성 없는 애들을 연관을 지어주기 위해 상위개념을 지어주는 것 -> 오렌지와 사과의 가격을 합해줌)   
>> 곱셈 : 연관성 요약(오렌지에 해당되는 가격을 갯수에 곱해서 총 오렌지 가격을 구함)

## 수열과 집합의 합과 곱
1. 수열(sequence)   
    - N개 숫자 또는 변수가 순서대로 나열된 것으로 문자에 붙은 아래 첨자는 순서를 나타내는 숫자로서 인덱스(index)라고 부름
      - index : 1, 2, 3으로 일반적으로 프로그래밍이지 않은것 
      - offset index : 첫번째 데이터에서 얼마나 떨어져 있는지 보는것으로 프로그래밍인 것(0, 1, 2, 3)
    -  너무 길면은 중간에 ...으로 생략할 수 있음
  
2. 집합(set)
   - 앞뒤로 {}가 붙음
   - 집합에서의 index는 id를 나타내 순서가 없는 고유값
   - 수학에서 가장많이 사용하는 집합이 실수 집합으로 $R$ 즉, 이세상에 존재하는 모든수가 들어있는 집합을 가장 많이 사용함
    $x ∈ R$
   - 두 개 이상의 숫자로 이뤄지면 집합의 크기도 당연히 달라짐
     - $R^n$으로 표현함


3. 수열의 합과 곱
   - 내가 가지고 있는 데이터를 더하거나 곱함
    1. 시그마($Σ$)      
    선형대수에서 많이 사용  
    2. 파이($Π$)   
    확률에서 많이 사용
   - 중첩 수식은 파이썬의 중첩 for문과 똑같음


## 데이터와 행렬

### 데이터의 유형
1. 스칼라(scalar)
   - 0차원 데이터(0-Rank T)
   - 하나의 숫자만으로 이뤄진 데이터
2. 벡터(vector)
   - 1차원 데이터(행, 열을 볼때 한방향으로만 데이터가 증가함함_1-Rank T)
     - 행 벡터(Row) : 한 개의 표본에 대한 n개의 데이터
     - 열 벡터(Column) : n개의 표본에 대한 한가지 데이터 
   - (예시) 수열의 유형 중 하나로 판다스의 시리즈로 볼 수 있음
3. 행렬(matrix)
   - 2차원 데이터(2-Rank T)
   - $N * M$ : $N$명에 대해서 $M$개의 정보를 구하기
   - 행렬로 스칼라를 1 * 1로, 벡터를 1 * n, n * 1로 표현 할수 있음   
   이게 왜 중요하냐면, 파이썬에서는 스칼라, 벡터를 처리하지 못해서 무조건 행렬로 나타내고 처리해야함
   <p align="center">
      <img src="../이미지/선형대수학01.png">
   </p>
   - (예시) 판다스의 데이터프레임
4. 텐서(tensor) 
   - 3차원 이상의 다차원 배열 데이터
   - (활용) 딥러닝시 많이 다루는 내용
   - 이를 통해 이미지의 유사도를 비교할 수 있음

### 전치 연산   
행렬에서 가장 기본이 되는 연산으로 행렬의 행과 열을 바꾸는 연산
- `대각선을 중심`으로 행과 열의 원소가 바뀐다 생각하면 편함 
- $X$라는 행렬이 있으면 보통 열벡터로 이해를 하여 $X^T$는 행 벡터로 이해
    <p align="center">
    <img src="../이미지/선형대수학02.png">
    </p>

### 특수한 벡터와 행렬
1. 영벡터   
모든 원소가 0
   - NumPy에서 일벡터 생성 = `ones()`

2. 정방행렬   
행의 개수와 열의 개수가 같은 행렬

3. 대각 행렬  
행렬에서 행과 열이 같은 위치
   - 모든 비대각 요소가 0인 행렬
   - NumPy에서 대각 정방행렬 생성 = `diag()`

4. 항등행렬    
대각행렬 중에서도 모든 대각성분의 값이 1인 대각행렬
   - 항등행렬은 보통 알파벳 대문자 `I`로 표기
   -  NumPy에서 대각 정방행렬 생성 = `identity()` or `eye()`

5. 대칭행렬   
전치연산을 통해서 얻은 전치행렬과 원래의 행렬이 같음
   - 정방행렬만 대칭행렬이 될 수 있음
 
## 벡터와 행렬의 연산

1. 벡터.행렬의 덧셈과 뺄셈
   - 연산을 할때 꼭 두 벡터의 모양은 일치해야함
   - 같은 위치에 있는 원소끼리 연산이 이뤄짐

2. 스칼라와 벡터.행렬의 곱셈
   - 스칼라값은 모든 벡터 원소에 분배되어 곱해짐 

3. **선형조합(linear combination)**(중요!)   
   - 벡터/행렬에 스칼라값을 곱한 후 더하거나 뺀 것
    > 하지만, 벡터나 행렬을 선형조합해도 크기는 변하지 않음. 즉, 차수가 변하지 않음

4. 벡터와 벡터의 곱셈($x^Ty$)
   - 정보와 정보의 조합으로 하나의 값으로 나타냄
   - 내적(inner product)혹은 닷 프로덕트(dot product)라고 불림   
   - 아래 코드를 보면 무조건 내적을 하는구나라고 생각해야함    
   $$x⋅y=<x,y>=x^Ty$$
   - 내적을 하기위해 두 조건을 만족해야함
     1. 두 벡터의 차원(길이)가 같아야함
     2. `ㅓ`의 형태로 앞의 벡터가 행 벡터이고 뒤의 벡터가 열 벡터여야 함($x^Ty$)
   - 같은 위치에 있는 원소들을 곱함
    > 여기서 잠깐! 그럼 앞이 열벡터이고 뒤가 행벡터인것은 뭘까?($xy^T$)
    >> 외적으로 불리며 `ㅏ`의 형태로 내적은 곱해서 더한다면 외적은 곱한뒤 나열만 한다
    

    외적 예시1) 길이가 같은 일벡터 1N∈RN와 행벡터 x∈RN의 곱은 행벡터 x를 반복하여 가지는 행렬과 같음을 보여라.
    $$
    \mathbf{1}_N^{} x^T
    = 
    \begin{aligned}
    \begin{bmatrix}
    {x}^T \\
    {x}^T \\
    \vdots    \\
    {x}^T \\
    \end{bmatrix}
    \end{aligned}
    $$

    풀이)
    $$ 
    \mathbf{1}_N^{}
    =
    \begin{bmatrix}
    1 \\
    1 \\
    \vdots \\
    1 \\
    \end{bmatrix},
    \quad
    x
    =
    \begin{bmatrix}
    x_{1} \\
    x_{2} \\
    \vdots \\
    x_{N} \\
    \end{bmatrix},
    \quad
    x^T
    =
    \begin{bmatrix}
    x_{1} & x_{2} & \cdots & x_{N} 
    \end{bmatrix}
    \\[10pt]
    \mathbf{1}_N^{} \in \mathbf{R}^{N \times 1},
    \quad
    x^T \in \mathbf{R}^{1 \times N}
    \;
    \rightarrow
    \;
    \mathbf{1}_N^{} x^T \in \mathbf{R}^{N \times N} 
    \\[20pt]
    \begin{aligned}
    \mathbf{1}_N^{} x^T
    &=
    \begin{bmatrix}
    1 \\
    1 \\
    \vdots \\
    1 \\
    \end{bmatrix}
    \begin{bmatrix}
    x_{1} & x_{2} & \cdots & x_{N} 
    \end{bmatrix}
    \\
    &=
    \begin{bmatrix}
    1 \times x_{1} & 1 \times x_{2} & \cdots & 1 \times x_{N} \\
    1 \times x_{1} & 1 \times x_{2} & \cdots & 1 \times x_{N} \\
    \vdots & \vdots & \ddots & \vdots \\
    1 \times x_{1} & 1 \times x_{2} & \cdots & 1 \times x_{N} \\
    \end{bmatrix}
    \\
    &=
    \begin{bmatrix}
    {x}^T \\
    {x}^T \\
    \vdots    \\
    {x}^T \\
    \end{bmatrix}
    \end{aligned}
    $$

   
5. 가중합($w^Tx$)
   - 내적처럼 단순히 곱하는 것이아닌, 가중치를 사용
     - **가중치(weight)** : 결과를 결정할 때 부과적인 정보을 의미하며 절대값이 클 수록 결과에 영향을 많이 미침. 고유의 값으로 변하지 않음
     (예시) 머신러닝에서는 가중치가 결국 학습대상임
    <p align="center">
    <img src="../이미지/선형대수학03.png">
    </p>


6. 유사도
   - 두 벡터가 닮은 정도를 정량적을 나타낸 값
   - 내적을 이용하면 **코사인 유사도(cosine similarity)**라는 유사도를 계산할 수 있음

7. 제곱합($x^Tx$)
   - 데이터의 분산(variance)이나 표준 편차(standard deviation) 등을 구하는 경우에는 각각의 데이터를 제곱한 뒤 이 값을 모두 더함


### 선형회귀 모형
독립변수 x에서 종속변수 y를 예측하는 것 
- $y=ax$
- 독립변수들간에는 서로간에 연관성이 전혀 없음
  - 하지만 두 변수간 상관계수가 너무 높으면 다중공선성이 나타날 수 있어서 독립변수를 넣을때 항상 신중해야함
- 계수($a$)가 가중치!
  
**[한계점]**
- 데이터가 많아질수록 항상 직선 위에 데이터가 있을 수많은 없음
- 즉, 비선형적인 관계를 보이는데, 선형회귀로는 현실적인 데이터를 100%를 반영하기에는 어려움
- 적당하게 반영하는 것이 최선이기에 `추세선`을 사용! 
  > 즉, 이 추세선을 잘 구할 수 있는 가중치를 구하는 것이 머신러닝의 목적

### 행렬과 행렬의 곱셈
$$A * B = C$$
- $C$의 $i$번째 행, $j$번째 열의 원소 $c_{ij}$의 값은 $A$ 행렬의 $i$번째 행 벡터 $a^T_i$와 $B$ 행렬의 $j$번째 열 벡터 $b_j$의 곱
  > 이 정의가 성립하기 위해 앞의 행렬 A의 열의 수가 뒤의 행렬 B의 행의 수와 일치해야함 
- 넘파이에서 계산할때 `@` 연산자 또는 `dot()` 사용

### 교환법칙 분배법칙
```
일반적인 교환, 분배법칙
덧셈의 교환법칙 : A + B = B + A
곱셈의 교환법칙 : AB = BA
덧셈의 분배법칙 : A(B + C) = AB + AC
뺄셈의 분배법칙 : A(B - C) = AB - AC
```
- 스칼라에서는 교환, 분배법칙 모두 가능
1. 행렬 내적에서는 교환법칙이 성립하지 않을 수 있음!
   - 앞의 행렬의 열과 뒤의 행렬의 행의 수가 다를 수 가 있기 때문
    > 이에, 곱셈의 교환법칙은 성립하지 않음 
    $$AB≠BA$$
2. 분배는 가능하지만 조심해야함
   - A가 앞에있으면 앞으로만 붙어야함
    $$
    A(B+C)=AB+AC\\
    A(B+C)≠BA+CA
    $$
3. 전치 연산
   - 덧셈/뺄셈에 대해 분배 법칙이 성립
   - 그러나! 분배과정에서 곱셈의 순서가 바뀌게 됨
    $$
    (A+B)^T=A^T+B^T \\
    (AB)^T=B^TA^T\\
    (ABC)^T=C^TB^TA^T
    $$

---
8. 항등행령의 곱셈
   - 어떤 행렬이든 항등행렬을 곱하면 그 행렬의 값이 변하지 않음
   $$AI=IA=A$$
   
9. 행렬과 벡터의 곱   
   행렬과 백터의 내적(곱)은 벡터
      $$
      x^T*A=x^T
      \\
      A*x=x
      $$
   
   (1) 열 벡터의 선형조합
      - 각 물품마다의 가격을 볼 수 있음
      - 기존 내적으로 계산을 할수 있지만 선형조합의 방식으로도 계산을 할 수 있음
      <p align="center">
      <img src="../이미지/선형대수학04.png">
      </p>
   
   (2)  여러 개의 벡터에 대한 가중합 동시 계산
      - 가중치는 데이터의 개수와는 상관이 없고 데이터의 종류만큼 가지고 있음. 즉, 열의 수만큼 있음
      - 여러개의 미지수가있고 거기에 대하 가중치를 한번에 계산을 하는 것 
    
10. 잔차(residual) or 오차(error)
    - 예측치와 실젯값(target) $y_i$의 차이
    - 잔찻값을 모든 독립변수 벡터에 대해 구하면 잔차 벡터 $e$
   $$ei=yi−y^i=yi−wTxi$$

11. 잔차 제곱합(RSS: Residual Sum of Squares)
    - 절대적 거리를 확인하기 위해 음수를 제거
    - 즉, 각 오차를 제곱하여 더함

12. 이차형식
    - $A$가 정방행렬 일때 $x^TAx$를 이차형식이라 함
    - 결국 모든 원소들의 곱의 합이 된다
      - 참고로 여기서 곱한다는 것은 같은 위치에 있는 원소를 말하는 것으로 
      - 예시) 행렬의 (1,3)에 위치한 원소가 10이면 1*3*10을 함. 그리고 행렬에 각 위치별로 이런식으로 다 곱해서 더하면 최종값이 나옴
      $$
      x^TAx=\sum_{i=1}^N\sum_{j=1}^Na_{i,j}x_ix_j
      $$


### (중요!)스칼라, 벡터, 행렬의 곱 공식
- 행벡터와 열벡터는 스칼라
- 행벡터와 행렬은 행벡터
- 행렬과 열 벡터는 열벡터
- 즉, 행력과 벡터의 곱은 벡터

### 부분행렬
2차 정방행렬 A, B는 부분행렬을 이용해 여러 방법으로 계산할 수 있음
1. 앞에 곱해지는 행렬을 행벡터로 나눠 게산함
2. 혹은 뒤에 곱해지는 행렬을 열벡터로 나눠 계산함
3. 혹은 마지막으로 앞에 곱해지는 행렬을 열벡터로, 뒤에 곱해지는 행렬을 행벡터로 나눠 스칼라처럼 계산해도 괜찮음

## 행렬의 성질

### 정부호와 준정부호
- 대칭행렬에 대해서만 정의함
- 이때 벡터 $x$는 영벡터가 아님($x \neq \mathbf{0}$). 
   - 벡터 $x$가 영벡터라면 모든 행렬(여기서는 $A$)는 항상 양의 준정부호가 되기 때문
1. 정부호(positive definite)
   - x는 절대 0벡터가 될 수 없음   
   $$x^TAx>0$$
2. 준정부호(positive semi-definite)
   - 0을 포함함   
   $$x^TAx\geq 0$$

### 행렬의 크기
1. 행렬 놈
   - 벡터나 행렬의 크기를 일반화 시킨 것
   - 우리는 p를 무조건 2를 사용 할 것. 그래서 p값이 표현안되어 있으면 걍 2구나 라고 생각할 것 
     - $L^2-Norm$=프로베니우스 놈(Frobenius norm)=원점과의 거리=유클리드 놈([참고](https://eehoeskrap.tistory.com/227))
   - 놈은 항상 0보다 크거나 같음
   - 당연히 모든 크기의 행려에 대해 정의가 가능하니 벡터에 대해서도 정의 할 수 있음
     - (중요!) 벡터의 놈의 제곱 = 벡터의 제곱합과 같음   
     $$\|x\|^2=\sum_{i=1}^Nx_i^2=x^Tx$$
   - 놈의 제곱이 가장 작을 때 놈도 가장 작음. 즉, 0일때 가장 작음
   - 4가지 성질
      1. 놈의 값은 0 이상
      2. 행렬에 스칼라를 곱하면 놈의 값도 스칼라의 절대값을 곱한 것과 같음
      3. 행렬의 합의 놈은 각 행렬의 놈의 합보다 작거나 같음
          - 음수가 있을 수 있어서
      4. 정방행렬의 곱의 놈은 각 정방행렬의 놈의 곱보다 작거나 같음
  > 즉, 오차제곱합을 잘 구하기 위해 봐야함

2. 대각합
   - 정방행렬에 대해서만 정의됨
   - 주 대각 방향에 있는 값을 다 더한 합. 한마디로 대각선의 값들의 합
   - 특징
      1. 더하기만 하다보니 음수가 될 수 있음
      2. 스칼라를 곱하면 대각합은 스칼라와 원래의 대각합의 곱
      3. 전치연산을 해도 대각합은 그대로
      4. 두 행렬의 합의 대각합은 두 행렬의 대각합의 합
      5. 두 행렬의 곱의 대각합은 행렬의 순서를 바꿔도 달라지지 않음
      6. 세 행렬의 곱의 대각합은 순서를 순환 시켜도 달라지지 않음
         $$tr(ABC)=tr(BCA)=tr(CAB)$$
   > 결론은 대각합은 거의 똑같음

3. 행렬식(determinant)
   - 정방행렬 A의 행렬식은 $det(A)$, $detA$, 또는 $|A|$라는 기호로 표기
   - 행렬A가 스칼라인 경우 행렬식이 자기 자신의 값으로 표현 됨
   - 스칼라가 아니라면, 행렬식은 더이상 나눠지지않을 정도로 스칼라의 값이 나올때 까지 계속해서 여인수 전개를 이용해 계산 하는 것으로 **재귀적**이라 할 수 있음
      $$
      \det(A)=\sum_{i=1}^N \{(-1)^{i+j_0}M_{i, j_0}\}a_{i, j_0}
      \\
      or
      \\
      \det(A)=\sum_{j=1}^N \{(-1)^{i_0+j}M_{i_0, j}\}a_{i_0, j}
      $$

       - $(-1)^{i+j_0}$ = 지정한 행/열의 원소 앞에 곱해지는 값(부호)
       - $a_{i, j_0}$ =  지정한 행/열의 원소
       - $M_{i, j_0}$ = 마이너(소행렬식)로 정방행렬 $A$에서 선택된 $i$, $j$를 삭제하고 얻어낸 행렬의 행렬식
   
   - 행렬식 풀이과정
     1. 행 또는 열 지정
     2. 식 전개
     3. 소 행렬식 전개 
     4. 각 소 행렬식에 대해 행렬식 또 전개
     5. 스칼라가 나올때 까지 무한루프
     6. 스칼라가 나오면 전체 더하기
   - 성질
     1. 전치 행렬의 행렬식은 원래의 행렬의 행렬식과 같다
     2. 항등 행렬의 행렬식은 1
     3. 두 행렬의 곱의 행렬식은 각 행렬의 행렬식의 곱과 같다
     4. 역행렬 $A^{−1}$은 원래의 행렬 A와 다음 관계를 만족하는 정방행렬을 말한다
     5. 행렬의 행렬식은 원래의 행렬의 행렬식의 역수와 같다
     6. 역행렬의 정의와 여인수 전개식을 사용하여 증명할 수 있다
     7. 즉, 역행렬의 판단여부를 확인할 수 있음
   - 2차 정방행렬의 행렬식 공식
      $$
      \det \left( \begin{bmatrix}a&b\\c&d\end{bmatrix} \right) = ad-bc
      $$

   
## 선형 연립방정식과 역행렬
입력 데이터 벡터와 가중치 벡터의 내적으로 계산된 예측값이 실제 출력 데이터와 유사한 값을 출력하도록 하는 모형
> 즉, 가중 벡터를 제외한 나머지 값은 다 알기에 연립방정식을 활용하여 가중 벡터값을 구함. 
>> 가감법, 대입법 사용. 하지만 데이터가 많아지면 한계가 있어서 역행렬을 사용

특히 행렬에서는 나눗셈의 개념이 없어 역행렬을 사용해 미지수의 값을 구해야함

### 역행렬
정방 행렬 A에 대한 역행렬(inverse matrix) $A{^−1}$은 원래의 행렬 A와 다음 관계를 만족하는 정방 행렬
1. 역행렬이 존재하는 행렬을 가역행렬(invertible matrix), 정칙행렬(regular matrix) 또는 비특이행렬(non-singular matrix)
2. 역행렬이 존재하지 않는 행렬을 비가역행렬(non-invertible matrix) 또는 특이행렬(singular matrix), 퇴화행렬(degenerate matrix)
- 역행렬의 성질
  1. 전치 행렬의 역행렬은 역행렬의 전치 행렬과 같음
  2. 대칭행렬의 역행렬도 대칭 행렬임
  3. 크기가 같은 정방행렬의 곱의 역행렬은 전치가 분배될때 순서가 바뀌는 것처럼 얘도 순서가 바뀌어서 나옴
  $$(AB)^-1=B^-1A^-1$$
- 역행렬의 계산
  1. det(A)이 0이 아니면 역행렬이 존재함 
   > det(A)를 벌써 까먹은거 아니지,,? 행렬식! ad-bc!!!
  2. 즉, 역행렬이 존재하면 연립방정식이 아예 풀지를 못함. 왜냐면 행렬을 넘기지를 못하기 때문
  어드조인트행렬은 공부할 필요가 없음
- 공식
  $$
   \begin{bmatrix}
   a_{11}&a_{12}\\
   a_{21}&a_{22}
   \end{bmatrix}^{-1} = \frac{1}{a_{11}a_{22}-a_{12}a_{21}}
   \begin{bmatrix}
   a_{22}&-a_{12}\\
   -a_{21}&a_{11}
   \end{bmatrix}
  $$
- 역행렬에 대한 정리(너무 깊에 공부하진 말고 아 이런게 있구나 정도로만 보기)
   1. 셔먼-모리슨 공식
   2. 우드베리 공식
   3. 분할행렬의 역행렬   
      - 내가 원하는 크기만큼 행렬을 잘라서 자른부분에 대해서 역행렬을 구하는것   
      - 이렇게 하면 100 by 100의 행렬처럼 데이터의 양이 많을 때 분할해서 효율적으로 역행렬을 구할 수 있음