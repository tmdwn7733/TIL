# 분류
Tree 알고리즘-> 분류    
선형 알고리즘 -> 회귀    
모든 알고리즘에 상관없이 분로, 회귀 알고리즘이 모두 존재하지만 우리가 짧은 시간에 모든 알고리즘을 배우기 어려우니 우리는 대표적인 알고리즘별 분류하는법 회귀하는법을 배울것

## 분류 알고리즘
학습 데이터로 주어진 데이터의 feature와 label 값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측   
- 종류
  - 베이즈 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)
  - 독립변수와 종속변수 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)
  - 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)
  - 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)
  - 근접 거리를 기준으로 하는 최소 근접 알고리즘 (Nearest Neighbor)
  - 심층 연결 기반의 신경망
  - 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)

## 결정 트리(이진 트리)
데이터에 있는 규칙을 학습을 통해 자동으로 찾아내는 트리 기반의 규칙 알고리즘
- 여기서 규칙이란 부등호를 의미
- 용어
  1. Root Node: 맨처음 규칙을 시작하는 질문지
  2. Rule Node: 분류를 하기위해 규칙을 만들어주는 새로운 질문지
  3. Leaf Node: 더이상 분류를 위한 규칙을 만들 질문지가 없음. 
     - 즉, 이미 질문지에 대해 답을 낸것으로 분류가 100% 된것
     - pure Node라고도 부름
  4. 브랜치/서브트리: 새로운 규칙 조건마다 규칙 노드 기반의 서브 트리 생성
  5. 깊이(Depth): 질문의 깊이로, 복잡하고 질문을 많이 할 수록 질문의 깊이가 깊어지는 것
- 균일도(순수도)를 측정하는 방법
  1. 정보 이득
     - 엔트로피(혼란도, 복잡도) 개념을 기반으로 분류
     - 서로 다른 값이 섞여 있으면 엔트로피가 높고 같은 값이 섞여 있으면 엔트로피가 낮음
     - 정보 이득 지수 = 1 - 엔트로피 지수
     - 결정 트리는 정보 이득 지수로 분할 기준을 정함. 즉, 정보 이득이 높은 속성을 기준으로 분할함
    (사진)
  2. 지니 계수   
  최근에는 정보 이득 보다는 지니 계수롤 더 많이 사용함
     - 경제학에서 불평등 지수를 나타낼 때 사용하는 용어로
     - 0일수록 공평하고 1일수록 혼잡함
     - 머신러닝에 적용될 때는 지니 계수가 낮을 수록 데이터 균일도가 높은 것으로 해석되어 계수가 낮은 속성을 기준으로 분할
- 결정 트리의 규칙 노드 생성 프로세스
  1. 데이터 집합의 모든 같이템이 같으 분로에 속하는지 확인
     - 노드 내 데이터가 균일한지를 확인, 지니계수가 0인지 확인하면서 시작
  2. 아래 둘 중하나를 선택함
     1. 데이터의 집합의 모든 아이템이 한 종류라면 리프 노드로 만들어서 분류 결정
       - 즉, 리프 노드는 하나의 종류로만 이루어진 노드로 더이상 질문지 생성이 불가능함
     2. 데이터의 집합의 아이템이 여러 종류라면 데이터를 분할하는데 가장 좋은 속성과 분할 기준(부등호)을 찾음(정보 이득 or 지니 계수)
  3. 해당 속성과 분할 기준으로 데이터를 분할하여 규칙 노드 생성
  - 1 -> 2.2 -> 3의 과정을 모든 데이터 집합의 분류가 결정 될 때 까지 반복함
- 장점
  - 쉽고 직관적인 알고리즘으로 시각화를 통해 모델의 학습을 보기 편함
  - feature의 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않음
- 단점
  - 과적합(Overfitting)으로 알고리즘 성능이 떨어짐
  - 이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝이 필요(a.k.a 하이퍼 파라미터)
(사진)
- petal length의 조건을 기준으로 나눔


### 과적합
- 과적합: 너무 완벽하게 분류를 하려고 하는것
(사진) 
파란색 안에 있는 빨간 점 하나를 제거하기 위해 파란색을 제외한 빈공간이 빨간색의 영역이 됨. 또한 빨간색 점 하나를 쳐내기 위해서 기존 파란색인 애가 빨간색으로 바뀌게 되는 현상이 됨
그래서 테스트 데이터의 정확도가 무조건 낮아질수 밖에 없음
> 너무 데이터에 초점을 둔 나머니 전체적인 추세를 보지 못하는 것

(사진)
모델 복잡도: 모델이 데이터 포이트를 얼마만큼 복잡하게 분류/회귀 하려고 노력하는지를 봄
  - 높을 수록 데이터를 하나하나 신경쓰고
  - 낮을 수록 데이터의 추세만을 봄
- DT에서 보면 훈련데이터에 대한 모델복잡도가 최상인것.
  - 그래서 훈련한 데이터의 정확도가 매우 높아서 사실 DT의 훈련데이터의 정확도는 100%가 될수밖에없음
  - 리프노드의 지니계수를 봐도 전부 0인걸 알 수 있음
- 그렇다고 해서 너무 데이터의 추세만 보기만 하면 과소적합이 될수 있음
> 과소와 과대의 적절한 부분인 일반화 지점을 찾을수 있게 노력해야함

### 하이퍼 파라미터
모델의 복잡도를 조절하기 위해 하이퍼 파라미터를 조절함
- 설정하는 법
  1. max_depth
    - 트리의 최대 깊이를 규정
    - 지정하지 않으면 DT는 계속 깊이가 깊어져 무조건 과대적합이 될수밖에 없음
    - 그렇지만, max_depth를 지정하면 적당한 선에서 지정하지 않기에 모델이 단순화 될수 있음
  2. min_samples_split
     - 노드가 분할되기 위한 최소한의 샘플 데이터 개수
     - Default는 2고, 작게 설정할 수록 분할되는 노드가 많아져 과적합 가능성이 높아짐
     - 즉 11개로 설정하면 노드의 데이터가 10개면 더이상 쪼개지지 않음
  - 1번과 2번을 같이 사용하는 경우도 많음
  - 1번과 2번이 가장 사용을 많이하고 아래 세개는 있구나 정도로만 알고 있기
  3. max_features
  4. min_samples_leaf
  5. max_leaf_nodes

## 앙상블 학습
여러 개의 분류기(Classifier)를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법
- 합적이고 어려운 문제의 결론을 내기 위해 각 분야 별 전문가들의 다양한 의견을 수렴하고 결정하듯이 앙상블 학습의 목표는 다양한 분류기의 예측 결과를 결합함으로써 단일 분류기보다 신뢰성이 높은 예측값을 얻는 것
- 유형
  배깅과 부스팅을 가장 많이 사용
  1. 보팅(Voting)
  2. 배깅(Bagging): 랜덤 포레스트
  3. 부스팅(Boosting): 에이다 부스팅, 그라디언트 부스팅, XGBoost, LightGBM
  4. 스태킹(Stacking)
   - 나중에 프로젝트 할때도 모델 하나만 사용한다 생각하지말것
    > 각 데이터세트에 맞는 모델이 있으니 앙상블이란 용어처럼 여러개의 모델을 적용해보고 뛰어난 성능을 보여주는 모델을 보여주기
- 단일 모델의 약점을 다수의 모델들을 결합하여 보완
- 뛰어난 성능을 가진 모델들로만 구성하는 것보다 성능이 떨어지더라도 서로 다른 유형의 모델을 섞는 것이 오히려 전체 성능에 도움이 됨
- 랜덤 포레스트 및 뛰어난 부스팅 알고리즘들은 모두 **결정 트리 알고리즘**을 기반 알고리즘으로 적용함
- 결정 트리의 단점이 과적합(오버피팅)을 수십~수천개의 많은 분류기를 결합해 보완하고 장점인 직관적인 분류 기준은 강화됨
> 즉, 혼자서 돌리는 것보다 여러가지 모델을 묶어서 분석하는게 뛰어남

### 보팅(Voting), 배깅(Bagging)
1. **보팅**   
여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정
- 보팅과 배깅의 차이
  - 보팅은 하나의 데이터세트에서 서로 다른 알고리즘을 가진 분류기를 결합
  - 배깅은 같은 알고리즘을 가진 분류기를 결합하지만 데이터 샘플링을 서로 다르게 수행하면서 학습을 수행해 보팅에 수행
    - 즉, 데이터를 쪼갠다음 같은 알고리즘을 사용
    - 행이 나눠지는게 아니라 feature를 나눠서 데이터를 쪼갬
- 보팅의 종류
  1. Hard Voting
    다수결 방식으로 예측하는 것
    (사진)
  2. Soft Voting
    다수의 Classifier 들의 class 확률을 평균으로 계산해서 계산함
    (사진)
**배깅**   
Bootstrap Sampling의 줄임말
  - Bootstrap이란 기존 학습 데이터 세트로 부터 랜덤하게 복원추출 하여 동일한 사이즈의 데이터 세트를 여러 개 만드는 것을 의미 -> 즉, `복원추출`방식
  - 이 방법이 어떻게 보면 앙상블의 개요하고 할 수 있을정도로 다양한 데이터들의 조합을 이용해 다양한 방향성을 띄우게 함
- **랜덤 포레스트(Random Forest)**: 배깅의 대표적인 알고리즘
Bootstrap을 통해 데이터를 Random하게, Tree를 모아 놓은 구조이기 때문에 숲(Forest)이 붙어 RandomForest라 불림
- 주요 하이퍼 파라미터
  1. n-estimators: 숲을 구성한 나무의 개수
    - 일반적으로 가장 많이 사용. 아래 두개는 음,, 고수들만 잘 볼수 있음
    - 나무 한개마다 부트스트래핑을 한번씩함. 
    - 기본은 100개
      - 그럼 많이 설정하면 좋은가? -> 노! 물론 많을수록 성능이 높을수도 있지만 무조건 향상되는건 아님
    - 트리의 개수가 많아서 학습 수행 시간이 오래 걸림
  2. max_features
  - max_depth나 min_samples_leaf와 같이 결정 트리에서 과적합을 개선하기 위해 사용되는 파라미터가 랜덤 포레스트에도 똑같이 적용될 수 있음

## 부스팅(Boosting)
부스팅 알고리즘은 여러 개의 약한 학습기(Weak Learner, **과소적합된 모델**)를 순차적으로 학습-예측한 데이터나 학습 트리에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식
- 부스팅의 대표적인 구현은 AdaBoost(Adaptive Boosting)과 그라디언트 부스트가 있음
  - 그러나, 이제 AdaBoost를 사용하지 않음 그저 그라디언트 부스트의 근간이 되는 거지 그라디언트 부스트만 사용!
- 즉, 랜덤포레스트가 과적합아이를 기준으로 분석한다면 부스팅은 과소적합된 모델을 순차적으로 다룬다는 차이가 있음

### GBM
adaBoost는 데이터에 가중치를 더하는거라면
GBM은 에러를 고치는 것에 있어서 가중치를 수정한다는 차이가 있음
- 경사하강법: 점점 오류를 낮추기 위해 위해 가중치를 갱신하는 기법
  - 미분을 해서 방향을 가는것으로 미분을 했을때 0이 되었을때 최적의 가중치로 봄
- 나무의 개수 = 학습의 개수
- learning_rate: 학습률, 절대 음수가 올수 없음.
  - GD를 갱신하는 법
  - 음수가 오게되면 내가 가는 방향의 반대쪽으로 가게 되는 경향이 잇음
- 단점으로는 다른 모델보다는 수행시간이 엄청오래걸림
- 그래서 실시간으로 보여주는 경우에는 오히려 그라디안 부스팅 머신을 안쓰는게 좋음
